README for `viterbi-bench`
==========================
viterbi-bench benchmarks a set of variations of the Viterbi
algorithm implemented in MRFy. viterbi-bench requires that
the MRFY_CHECKOUTS_PATH is set to a directory containing
git checkouts of MRFy branches. By default, viterbi-bench
recursively searches the directory for checkouts and runs
the benchmark specified against all of them.

The -b option can be used to specify only some branches to
benchmark.

viterbi-bench must be run inside the mrfy-bench directory.

protip: viterbi-bench -col [ args ]

Using valgrind:

  viterbi-bench -valgrind -- --leak-check=full
         
-smurf     When set, "smurf" is run on the benchmark in
           to mrfy.
           The SMURF_PATH environment variable must be
           set to the directory containing the "smurf"
           executable and the beta "dat" files.

-s         Benchmark that takes 5-10 seconds.

-m         Benchmark that takes 2 minutes and reports the
           better of two passes for each template.

-l         Benchmark that takes a significant amount of time.
           Reports the best of five passes for each template.

-valgrind  A benchmark small enough to be profiled by valgrind.
           When set, "valgrind" is run with options provided
           after "--". (See example above.)

-b "dir-name [ dir-name [ ... ] ]"
           A single string with a space delimited list of
           directory names to run benchmarks on.

-v         Show extra output, like the times for each HMM.

-col       Pipes output through "column".

-h         Show this usage information.


README for `mrfy-bench`
=======================
This is a simple benchmarking framework for MRFy.
It is not yet intended for public consumption.
There are four basic variants: `mrfy-bench <-s | -m | -l | -p>`
Short is intended as a microbenchmark; it takes 5-10 seconds on reasonable 
hardware.
Medium takes less than 2 minutes, and takes the better of two passes for each.
Long and Paper take a significant amount of time, running more templates and
choosing the best of 5 runs for each.
The only difference between Long and Paper is that the latter requires SMURF
to be run as well; Long only runs SMURF if either the SMURF_DAT_PATH environment
variable is set, or the path is passed as an argument after `-p`.
Medium runs SMURF under the same criteria as Long.
Note: mrfy and smurf (if it is being run) must be in your path.


